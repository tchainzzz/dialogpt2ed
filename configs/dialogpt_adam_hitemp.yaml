name: "microsoft/DialoGPT-medium" # change this when using pretrained
model: 
    optimizer:
        name: "Adam"
        kwargs:
            lr: 1.0e-3
    inference:
        temperature: 1.5
        top_k: 20
        top_p: 0.9
        min_length: 1
        max_length: 1000
    train:
        lm_weight: 2.0
        mc_weight: 1.0
data:
    name: "./data/empatheticdialogues/ed_merged.json"
    batch_size: 1
    tokenizer: "microsoft/DialoGPT-medium" # change this when using pretrained
    dataset_cache: "./data/empatheticdialogues/ed_cache.bin"
    num_candidates: 2
    max_history: 2
callbacks:
    ModelCheckpoint:
        monitor: 'val_loss'
        save_top_k: 1
        verbose: True
    EarlyStopping:
        monitor: 'val_loss'
        mode: 'min'
        verbose: True
logger:
    name: "dialogpt_ed_finetune_adam1e-3"
    project: "dialogpt2ed"

