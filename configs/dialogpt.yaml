name: "microsoft/DialoGPT-medium" # change this when using pretrained
model: 
    optimizer:
        name: "AdamW"
        kwargs:
            lr: 2.5e-4
            weight_decay: 0.1
            correct_bias: True
    inference:
        temperature: 0.7
        top_k: 0
        top_p: 0.9
        min_length: 1
        max_length: 20
        num_return_sequences: 10
    train:
        lm_weight: 2.0
        mc_weight: 1.0
data:
    name: "./data/empatheticdialogues/ed_merged.json"
    batch_size: 1
    tokenizer: "microsoft/DialoGPT-medium" # change this when using pretrained
    dataset_cache: "./data/empatheticdialogues/ed_cache.bin"
    num_candidates: 2
    max_history: 2
callbacks:
    ModelCheckpoint:
        monitor: 'val_loss'
        save_top_k: 1
        verbose: True
    EarlyStopping:
        monitor: 'val_loss'
        mode: 'min'
        verbose: True
logger:
    name: "dialogpt_ed_finetune"
    project: "dialogpt2ed"

