name: "microsoft/DialoGPT-medium" # change this when using pretrained
model: 
    optimizer:
        name: "AdamW"
        kwargs:
            lr: 6.25e-5
            weight_decay: 0.1
            correct_bias: True
    scheduler:
        epochs: 3
        steps_per_epoch: 41254
        pct_start: 0.
        anneal_strategy: "linear"
    inference:
        temperature: 0.7
        top_k: 0
        top_p: 0.9
        min_length: 1
        max_length: 20
        do_sample: True
        num_return_sequences: 1 #  save validation time
    train:
        lm_weight: 1.0
        mc_weight: 1.0
data:
    name: "./data/empatheticdialogues/ed_merged.json"
    batch_size: 1
    tokenizer: "microsoft/DialoGPT-medium" # change this when using pretrained
    dataset_cache: "./data/empatheticdialogues/ed_cache.bin"
    num_candidates: 2
    max_history: 2
    num_workers: 8
callbacks:
    ModelCheckpoint:
        monitor: 'val_loss'
        save_top_k: 3
        verbose: True
    EarlyStopping:
        monitor: 'val_loss'
        mode: 'min'
        verbose: True
logger:
    name: "dialogpt_ed_finetune"
    project: "dialogpt2ed"

