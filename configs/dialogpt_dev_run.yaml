name: "microsoft/DialoGPT-medium" # change this when using pretrained
model: 
    optimizer:
        name: "AdamW"
        kwargs:
            lr: 1.e-6 
    inference:
        temperature: 0.7
        top_k: 20
        top_p: 0.9
        min_length: 1
        max_length: 1000
    train:
        lm_weight: 2.0
        mc_weight: 1.0
data:
    name: "./data/empatheticdialogues/ed_merged.json"
    batch_size: 1
    tokenizer: "microsoft/DialoGPT-medium" # change this when using pretrained
    dataset_cache: "./data/empatheticdialogues/ed_cache.bin"
    num_candidates: 2
    max_history: 2
    num_workers: 0
logger:
    name: "dialogpt_ed_finetune"
    project: "dialogpt2ed"

